{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/titanic_meme.jpg)\n",
    "\n",
    "# Explaining model predictions with Shapley values - Random Forest\n",
    "\n",
    "The notebook below is a modified version of the original by Mike Allen's [Titanic Notebooks](https://github.com/MichaelAllen1966/titanic).\n",
    "\n",
    "Shapley values provide an estimate of how much any particular feature influences the model decision (prediction). When Shapley values are averaged they provide a measure of the overall influence of a feature.\n",
    "\n",
    "Shapley values may be used across model types, and so provide a *model-agnostic* measure of a feature's influence. This means that the influence of features may be compared across model types, and it allows *black box* models like neural networks to be explained, at least in part.\n",
    "\n",
    "Here we will demonstrate Shapley values with random forests.\n",
    "\n",
    "For more on Shapley values in general see Chris Molner's excellent book chapter:\n",
    "\n",
    "https://christophm.github.io/interpretable-ml-book/shapley.html\n",
    "\n",
    "The `shap` package is installed if you have used the Titanic environment yasml file, but otherwise may be installed with `pip install shap`.\n",
    "\n",
    "More information on the `shap` library, inclusiong lots of useful examples may be found at: https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Here we provide an example of using `shap` with Random Forests.\n",
    "\n",
    "Shap values are returned in a slightly different way to logistic regression  - there is a set of Shap values for each classification probablility ('not survive', 'survive') so we need slightly different syntax to access and use the Shap values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and fit model\n",
    "\n",
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import machine learning methods\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import shap for shapley values\n",
    "import shap # `! pip install shap` if neeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The section below downloads pre-processed data, and saves it to a subfolder (from where this code is run).\n",
    "If data has already been downloaded that cell may be skipped.\n",
    "\n",
    "Code that was used to pre-process the data ready for machine learning may be found at:\n",
    "https://github.com/MichaelAllen1966/1804_python_healthcare/blob/master/titanic/01_preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into X (features) and y (labels)\n",
    "\n",
    "We will separate out our features (the data we use to make a prediction) from our label (what we are truing to predict).\n",
    "By convention our features are called `X` (usually upper case to denote multiple features), and the label (survived or not) `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `survived` field as y, and drop for X\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "\n",
    "# Drop PassengerId\n",
    "X.drop('PassengerId',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into training and tets sets\n",
    "\n",
    "When we test a machine learning model we should always test it on data that has not been used to train the model.\n",
    "We will use sklearn's `train_test_split` method to randomly split the data: 75% for training, and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100,\n",
    "                               n_jobs=-1,\n",
    "                               class_weight='balanced',\n",
    "                               random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict values and get probabilities of survival\n",
    "\n",
    "Now we can use the trained model to predict survival. We will test the accuracy of both the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Predict probabilities of survival\n",
    "y_prob_train = model.predict_proba(X_train)\n",
    "y_prob_test = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy\n",
    "\n",
    "In this example we will measure accuracy simply as the proportion of passengers where we make the correct prediction. In a later notebook we will look at other measures of accuracy which explore false positives and false negatives in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train:0.3f}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the model importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X_train)\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "importances = pd.DataFrame(index=features)\n",
    "importances['importance'] = feature_importances\n",
    "importances['rank'] = importances['importance'].rank(ascending=False).values\n",
    "importances.sort_values('rank').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three most influential features are:\n",
    "\n",
    "* *male*\n",
    "* *Fare*\n",
    "* *age*\n",
    "\n",
    "Note: random forest importances do not tell us anything about the direction of effect of features (as with random forests, the direction of effect may depend on the value oif other features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Shapley values\n",
    "\n",
    "We use the `shap_values` method from the SHAP library to get Shapley values.\n",
    "We use the `explainer` method from the SHAP library to get Shapley values along with other data.\n",
    "\n",
    "We pass a sample to the explainer to speed up Shap (which can be slow with random forests - these values are used as expected baseline values for features).\n",
    "\n",
    "[`check_additivity`](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.TreeExplainer.html?highlight=check_additivity) has been disabled as the fit reported a small differnce between between predicted probability and baseline probability with all Shap values summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of features\n",
    "features = list(X_train)\n",
    "\n",
    "# Train explainer on Training set\n",
    "explainer = shap.TreeExplainer(model,\n",
    "                               X_train.sample(100))\n",
    "    \n",
    "# Get Shap values (extended version has other data returned as well as shap values)\n",
    "shapley_values_train_extended = explainer(X_train, check_additivity=False)\n",
    "shapley_values_train = shapley_values_train_extended.values[:,:,1]\n",
    "\n",
    "shapley_values_test_extended = explainer(X_test, check_additivity=False)\n",
    "shapley_values_test = shapley_values_test_extended.values[:,:,1]\n",
    "\n",
    "# Calculate mean Shapley value for each feature in training set\n",
    "importances['mean_shapley_values'] = np.mean(shapley_values_train, axis=0)\n",
    "\n",
    "# Calculate mean absolute Shapley value for each feature in training set\n",
    "# This will give us the average importance of each feature\n",
    "importances['mean_abs_shapley_values'] = np.mean(\n",
    "    np.abs(shapley_values_train),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Shapley values to coefficient table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.sort_values(by='importance', ascending=False).head()\n",
    "importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get top 10 influenctial features by co-effieceints or Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 features\n",
    "importance_top_10 = \\\n",
    "    importances.sort_values(by='importance', ascending=False).head(10).index\n",
    "shapley_top_10 = \\\n",
    "    importances.sort_values(\n",
    "    by='mean_abs_shapley_values', ascending=False).head(10).index\n",
    "\n",
    "# Add to DataFrame\n",
    "top_10_features = pd.DataFrame()\n",
    "top_10_features['importances'] = importance_top_10.values\n",
    "top_10_features['Shapley'] = shapley_top_10.values\n",
    "\n",
    "# Display\n",
    "top_10_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a lot of overlap between the most import fatures as estimated by coefficients and those estimated using mean absolute Shapley values. But they are not identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot comparison of Shapley and model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot points\n",
    "x = importances['importance']\n",
    "y = importances['mean_abs_shapley_values']\n",
    "\n",
    "ax.scatter(x, y)\n",
    "ax.set_title('Shapley value vs model weight (coefficient) for each feature')\n",
    "ax.set_ylabel('Mean absolute Shapley value')\n",
    "ax.set_xlabel('Feature importance')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of Shapley values\n",
    "\n",
    "### [Summary Plot](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html)\n",
    "\n",
    "The `summary_plot` using a `plot_type` option of `bar` gives us the overall importance of each feature across the population.\n",
    "\n",
    "Here we limit the num,ber of features shown to 15 (default is 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "shap.summary_plot(shap_values = shapley_values_train, \n",
    "                  features = X_train.values,\n",
    "                  feature_names = X_train.columns.values,\n",
    "                  plot_type='bar',\n",
    "                  max_display=15,\n",
    "                  show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Plot - Beeswarm\n",
    "\n",
    "Without specifying a `plot_type` option of `bar`, `summary_plot` gives us a *beeswarm* plot, showing the Shapley values for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "shap.summary_plot(shap_values = shapley_values_train, \n",
    "                  features = X_train.values,\n",
    "                  feature_names = X_train.columns.values,\n",
    "                  max_display=15,\n",
    "                  show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Waterfall Plot](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.waterfall_plot.html?highlight=plots.waterfall)\n",
    "\n",
    "Waterfall plots show the influence of  individual features on model prediction. These are shown as the effect on [*log odds ratio*](https://www.statisticshowto.com/log-odds/) of survival. *Log odds ratio* are usually shown as these are additive, whereas probabilities are not.\n",
    "\n",
    "Waterfall plots put the most influential features at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get locations of passengers with low or high probability of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Get the location of an example each where porbability of survival\n",
    "# is <0.01 or >0.99\n",
    "\n",
    "location_low_probability = np.where(y_prob < 0.05)[0][0]\n",
    "location_high_probability = np.where(y_prob > 0.95)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall plot for passenger with lowest probability of survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shapley_values_test_extended[location_low_probability][:,1], \n",
    "                     max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waterfall plot for passenger with high probability of survival:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shapley_values_test_extended[location_high_probability][:,1], \n",
    "                     max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Scatter Plot](https://shap-lrjball.readthedocs.io/en/latest/example_notebooks/plots/scatter.html)\n",
    "\n",
    "A scattter plot for one or more features shows the relationship between the feature value and the Shap value, along with a histogram of the frequency of the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_to_show = shapley_top_10[0:4]\n",
    "\n",
    "for feat in feat_to_show:\n",
    "    shap.plots.scatter(shapley_values_test_extended[:, feat][:,1],\n",
    "                       x_jitter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: unlike a logistic regression model, Shap values are not linearly releated to feature values. This is because of the more flexible classification method in random forests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
